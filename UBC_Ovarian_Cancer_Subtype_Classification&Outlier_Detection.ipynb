{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c0b3ee",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The goal of the UBC Ovarian Cancer subtypE clAssification and outlier detectioN (UBC-OCEAN) competition is to classify ovarian cancer subtypes. You will build a model trained on the world's most extensive ovarian cancer dataset of histopathology images obtained from more than 20 medical centers.\n",
    "\n",
    "Your work will help enhance the applicability and accessibility of accurate ovarian cancer diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32976786",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Ovarian carcinoma is the most lethal cancer of the female reproductive system. There are five common subtypes of ovarian cancer: high-grade serous carcinoma, clear-cell ovarian carcinoma, endometrioid, low-grade serous, and mucinous carcinoma. Additionally, there are several rare subtypes (\"Outliers\"). These are all characterized by distinct cellular morphologies, etiologies, molecular and genetic profiles, and clinical attributes. Subtype-specific treatment approaches are gaining prominence, though first requires subtype identification, a process that could be improved with data science.\n",
    "\n",
    "Currently, ovarian cancer diagnosis relies on pathologists to assess subtypes. However, this presents several challenges, including disagreements between observers and the reproducibility of diagnostics. Furthermore, underserved communities often lack access to specialist pathologists, and even well-developed communities face a shortage of pathologists with expertise in gynecologic malignancies.\n",
    "\n",
    "Deep learning models have exhibited remarkable proficiency in analyzing histopathology images. Yet challenges still exist, such as the need for a significant amount of training data, ideally from a single source. Technical, ethical, and financial constraints, as well as confidentiality concerns, make training a challenge. In this competition, you will have access to the most extensive and diverse ovarian cancer dataset of histopathology images from more than 20 centers across four continents.\n",
    "\n",
    "![ubc1](https://storage.googleapis.com/kaggle-media/competitions/UBC/OCEAN-Optional-Figure.png)\n",
    "\n",
    "Competition host University of British Columbia (UBC) is a global center for teaching, learning, and research, consistently ranked among the top 20 public universities in the world. UBC embraces innovation and transforms ideas into action. Since 1915, UBC has been opening doors of opportunity for people with the curiosity, drive, and vision to shape a better world. Joining UBC is the BC Cancer, part of the Provincial Health Services Authority and its world-renowned Ovarian Cancer Research (OVCARE) team whose discoveries have led to progressive prevention strategies and improved diagnostics and treatments. BC Cancer provides a comprehensive cancer control program for the people of British Columbia in partnership with regional health authorities. We have also partnered with the Ovarian Tumour Tissue Analysis (OTTA) consortium, an international multidisciplinary network of investigators from more than 65 international teams across the globe. Finally, the OCEAN challenge is made possible through a generous donation from TD Bank Group through the BC Cancer Foundation.\n",
    "\n",
    "Your work could yield improved accuracy in identifying ovarian cancer subtypes. Better classification would enable clinicians to formulate personalized treatment strategies regardless of geographic location. This targeted approach has the potential to enhance treatment efficacy, reduce adverse effects, and ultimately contribute to better patient outcomes for those diagnosed with this deadly cancer.\n",
    "\n",
    "Using the data outside of the competition\n",
    "\n",
    "We request that participants refrain from utilizing the competition data, either in its entirety or in part, for any external projects, research, or applications until the official publication of the competition paper. We will ensure that participants receive prompt notification when the embargo is lifted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742d7d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26908e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:07.235061Z",
     "iopub.status.busy": "2023-12-07T04:27:07.234305Z",
     "iopub.status.idle": "2023-12-07T04:27:16.652192Z",
     "shell.execute_reply": "2023-12-07T04:27:16.651061Z"
    },
    "papermill": {
     "duration": 9.429844,
     "end_time": "2023-12-07T04:27:16.654732",
     "exception": false,
     "start_time": "2023-12-07T04:27:07.224888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from numba import cuda\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72dc069c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:16.671083Z",
     "iopub.status.busy": "2023-12-07T04:27:16.670635Z",
     "iopub.status.idle": "2023-12-07T04:27:16.770435Z",
     "shell.execute_reply": "2023-12-07T04:27:16.769474Z"
    },
    "papermill": {
     "duration": 0.110064,
     "end_time": "2023-12-07T04:27:16.772471",
     "exception": false,
     "start_time": "2023-12-07T04:27:16.662407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 2023,\n",
    "    \"img_size\": 2048,\n",
    "    \"num_classes\": 5,\n",
    "    \"batch\": 1,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"lr\": 0.0005,\n",
    "    \"Threshold\" : 0.3\n",
    "}\n",
    "training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61dfcf84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:16.788518Z",
     "iopub.status.busy": "2023-12-07T04:27:16.788173Z",
     "iopub.status.idle": "2023-12-07T04:27:16.792793Z",
     "shell.execute_reply": "2023-12-07T04:27:16.791883Z"
    },
    "papermill": {
     "duration": 0.014666,
     "end_time": "2023-12-07T04:27:16.794614",
     "exception": false,
     "start_time": "2023-12-07T04:27:16.779948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kaggle/input/UBC-OCEAN'\n",
    "TEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\n",
    "ALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\n",
    "Train_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\n",
    "ALT_Train_DIR = '/kaggle/input/UBC-OCEAN/train_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153b9b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:16.810219Z",
     "iopub.status.busy": "2023-12-07T04:27:16.809939Z",
     "iopub.status.idle": "2023-12-07T04:27:16.815472Z",
     "shell.execute_reply": "2023-12-07T04:27:16.814624Z"
    },
    "papermill": {
     "duration": 0.015378,
     "end_time": "2023-12-07T04:27:16.817292",
     "exception": false,
     "start_time": "2023-12-07T04:27:16.801914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_file_path(image_id):\n",
    "    if os.path.exists(f\"{TEST_DIR}/{image_id}_thumbnail.png\"):\n",
    "        return str(f\"{TEST_DIR}/{image_id}_thumbnail.png\")\n",
    "    else:\n",
    "        return str(f\"{ALT_TEST_DIR}/{image_id}.png\")\n",
    "    \n",
    "def get_train_file_path(image_id):\n",
    "    if os.path.exists(f\"{Train_DIR}/{image_id}_thumbnail.png\"):\n",
    "        return str(f\"{Train_DIR}/{image_id}_thumbnail.png\")\n",
    "    else:\n",
    "        return str(f\"{ALT_Train_DIR}/{image_id}.png\")\n",
    "    #return str(f\"{ALT_Train_DIR}/{image_id}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c67fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:16.832581Z",
     "iopub.status.busy": "2023-12-07T04:27:16.832280Z",
     "iopub.status.idle": "2023-12-07T04:27:16.868337Z",
     "shell.execute_reply": "2023-12-07T04:27:16.867498Z"
    },
    "papermill": {
     "duration": 0.045958,
     "end_time": "2023-12-07T04:27:16.870342",
     "exception": false,
     "start_time": "2023-12-07T04:27:16.824384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>28469</td>\n",
       "      <td>16987</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/test_thumbnails/41_thu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  image_width  image_height  \\\n",
       "0        41        28469         16987   \n",
       "\n",
       "                                           file_path  label  \n",
       "0  /kaggle/input/UBC-OCEAN/test_thumbnails/41_thu...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\n",
    "test_df['file_path'] = test_df['image_id'].apply(get_test_file_path)\n",
    "test_df['label'] = 0 # dummy\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8ad457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:16.886037Z",
     "iopub.status.busy": "2023-12-07T04:27:16.885789Z",
     "iopub.status.idle": "2023-12-07T04:27:16.896034Z",
     "shell.execute_reply": "2023-12-07T04:27:16.895227Z"
    },
    "papermill": {
     "duration": 0.020271,
     "end_time": "2023-12-07T04:27:16.897956",
     "exception": false,
     "start_time": "2023-12-07T04:27:16.877685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>HGSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label\n",
       "0        41  HGSC"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.read_csv(f\"{ROOT_DIR}/sample_submission.csv\")\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adfeb55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:16.914368Z",
     "iopub.status.busy": "2023-12-07T04:27:16.914104Z",
     "iopub.status.idle": "2023-12-07T04:27:17.352846Z",
     "shell.execute_reply": "2023-12-07T04:27:17.351950Z"
    },
    "papermill": {
     "duration": 0.449384,
     "end_time": "2023-12-07T04:27:17.354859",
     "exception": false,
     "start_time": "2023-12-07T04:27:16.905475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HGSC' 'LGSC' 'EC' 'CC' 'MC']\n",
      "{0: 'CC', 1: 'EC', 2: 'HGSC', 3: 'LGSC', 4: 'MC', 5: 'Others'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/4_thu...</td>\n",
       "      <td>HGSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/66_th...</td>\n",
       "      <td>LGSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_images/91.png</td>\n",
       "      <td>HGSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>3</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/281_t...</td>\n",
       "      <td>LGSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>1</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/286_t...</td>\n",
       "      <td>EC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>65022</td>\n",
       "      <td>3</td>\n",
       "      <td>53355</td>\n",
       "      <td>46675</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65022...</td>\n",
       "      <td>LGSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>65094</td>\n",
       "      <td>4</td>\n",
       "      <td>55042</td>\n",
       "      <td>45080</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65094...</td>\n",
       "      <td>MC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>65300</td>\n",
       "      <td>2</td>\n",
       "      <td>75860</td>\n",
       "      <td>27503</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65300...</td>\n",
       "      <td>HGSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>65371</td>\n",
       "      <td>2</td>\n",
       "      <td>42551</td>\n",
       "      <td>41800</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65371...</td>\n",
       "      <td>HGSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>65533</td>\n",
       "      <td>2</td>\n",
       "      <td>45190</td>\n",
       "      <td>33980</td>\n",
       "      <td>False</td>\n",
       "      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65533...</td>\n",
       "      <td>HGSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id  label  image_width  image_height  is_tma  \\\n",
       "0           4      2        23785         20008   False   \n",
       "1          66      3        48871         48195   False   \n",
       "2          91      2         3388          3388    True   \n",
       "3         281      3        42309         15545   False   \n",
       "4         286      1        37204         30020   False   \n",
       "..        ...    ...          ...           ...     ...   \n",
       "533     65022      3        53355         46675   False   \n",
       "534     65094      4        55042         45080   False   \n",
       "535     65300      2        75860         27503   False   \n",
       "536     65371      2        42551         41800   False   \n",
       "537     65533      2        45190         33980   False   \n",
       "\n",
       "                                             file_path label_name  \n",
       "0    /kaggle/input/UBC-OCEAN/train_thumbnails/4_thu...       HGSC  \n",
       "1    /kaggle/input/UBC-OCEAN/train_thumbnails/66_th...       LGSC  \n",
       "2          /kaggle/input/UBC-OCEAN/train_images/91.png       HGSC  \n",
       "3    /kaggle/input/UBC-OCEAN/train_thumbnails/281_t...       LGSC  \n",
       "4    /kaggle/input/UBC-OCEAN/train_thumbnails/286_t...         EC  \n",
       "..                                                 ...        ...  \n",
       "533  /kaggle/input/UBC-OCEAN/train_thumbnails/65022...       LGSC  \n",
       "534  /kaggle/input/UBC-OCEAN/train_thumbnails/65094...         MC  \n",
       "535  /kaggle/input/UBC-OCEAN/train_thumbnails/65300...       HGSC  \n",
       "536  /kaggle/input/UBC-OCEAN/train_thumbnails/65371...       HGSC  \n",
       "537  /kaggle/input/UBC-OCEAN/train_thumbnails/65533...       HGSC  \n",
       "\n",
       "[538 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\n",
    "train_df['file_path'] = train_df['image_id'].apply(get_train_file_path)\n",
    "print(train_df['label'].unique())\n",
    "train_df['label_name']=train_df['label']\n",
    "train_df['label'] = train_df['label'].astype('category').cat.codes\n",
    "label_mapping = dict(enumerate(train_df['label_name'].astype('category').cat.categories))\n",
    "label_mapping[5] = 'Others'\n",
    "print(label_mapping)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6acc1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.372864Z",
     "iopub.status.busy": "2023-12-07T04:27:17.372274Z",
     "iopub.status.idle": "2023-12-07T04:27:17.385657Z",
     "shell.execute_reply": "2023-12-07T04:27:17.384761Z"
    },
    "papermill": {
     "duration": 0.02437,
     "end_time": "2023-12-07T04:27:17.387609",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.363239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cropped_images(file_path, image_id, label_t,th_area = 1000):\n",
    "    image = Image.open(file_path)\n",
    "    # Aspect ratio\n",
    "    as_ratio = image.size[0] / image.size[1]\n",
    "    \n",
    "    sxs, exs, sys, eys = [],[],[],[]\n",
    "    if as_ratio >= 1.5:\n",
    "        # Crop\n",
    "        mask = np.max( np.array(image) > 0, axis=-1 ).astype(np.uint8)\n",
    "        retval, labels = cv2.connectedComponents(mask)\n",
    "        if retval >= as_ratio:\n",
    "            x, y = np.meshgrid( np.arange(image.size[0]), np.arange(image.size[1]) )\n",
    "            for label in range(1, retval):\n",
    "                area = np.sum(labels == label)\n",
    "                if area < th_area:\n",
    "                    continue\n",
    "                xs, ys= x[ labels == label ], y[ labels == label ]\n",
    "                sx, ex = np.min(xs), np.max(xs)\n",
    "                cx = (sx + ex) // 2\n",
    "                crop_size = image.size[1]\n",
    "                sx = max(0, cx-crop_size//2)\n",
    "                ex = min(sx + crop_size - 1, image.size[0]-1)\n",
    "                sx = ex - crop_size + 1\n",
    "                sy, ey = 0, image.size[1]-1\n",
    "                sxs.append(sx)\n",
    "                exs.append(ex)\n",
    "                sys.append(sy)\n",
    "                eys.append(ey)\n",
    "        else:\n",
    "            crop_size = image.size[1]\n",
    "            for i in range(int(as_ratio)):\n",
    "                sxs.append( i * crop_size )\n",
    "                exs.append( (i+1) * crop_size - 1 )\n",
    "                sys.append( 0 )\n",
    "                eys.append( crop_size - 1 )\n",
    "    else:\n",
    "        # Not Crop (entire image)\n",
    "        sxs, exs, sys, eys = [0,],[image.size[0]-1],[0,],[image.size[1]-1]\n",
    "\n",
    "    df_crop = pd.DataFrame()\n",
    "    df_crop[\"image_id\"] = [image_id] * len(sxs)\n",
    "    df_crop[\"file_path\"] = [file_path] * len(sxs)\n",
    "    df_crop[\"sx\"] = sxs\n",
    "    df_crop[\"ex\"] = exs\n",
    "    df_crop[\"sy\"] = sys\n",
    "    df_crop[\"ey\"] = eys\n",
    "    df_crop[\"label\"] = [label_t] * len(sxs)\n",
    "    return df_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929a67b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.405219Z",
     "iopub.status.busy": "2023-12-07T04:27:17.404931Z",
     "iopub.status.idle": "2023-12-07T04:27:17.414960Z",
     "shell.execute_reply": "2023-12-07T04:27:17.414019Z"
    },
    "papermill": {
     "duration": 0.021127,
     "end_time": "2023-12-07T04:27:17.416951",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.395824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    222\n",
      "1    124\n",
      "0     99\n",
      "3     47\n",
      "4     46\n",
      "Name: count, dtype: int64\n",
      "{2: 0.4846846846846847, 1: 0.867741935483871, 0: 1.0868686868686868, 3: 2.2893617021276595, 4: 2.3391304347826085}\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_df is your DataFrame and 'label' is the column with class labels\n",
    "labels = train_df['label'].value_counts()\n",
    "print(labels)\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_samples = np.sum(labels)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(labels)\n",
    "\n",
    "# Calculating weights for each class\n",
    "weight_balance = {}\n",
    "for label, count in labels.items():\n",
    "    weight_balance[label] = total_samples / (num_classes * count)\n",
    "\n",
    "print(weight_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069ff218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.434215Z",
     "iopub.status.busy": "2023-12-07T04:27:17.433946Z",
     "iopub.status.idle": "2023-12-07T04:27:17.439367Z",
     "shell.execute_reply": "2023-12-07T04:27:17.438531Z"
    },
    "papermill": {
     "duration": 0.016355,
     "end_time": "2023-12-07T04:27:17.441396",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.425041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_df_tma = train_df[train_df['is_tma']==True]\\ntrain_df_no_tma = train_df[train_df['is_tma']==False]\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_df_tma = train_df[train_df['is_tma']==True]\n",
    "train_df_no_tma = train_df[train_df['is_tma']==False]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ce8e7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.459662Z",
     "iopub.status.busy": "2023-12-07T04:27:17.459090Z",
     "iopub.status.idle": "2023-12-07T04:27:17.472385Z",
     "shell.execute_reply": "2023-12-07T04:27:17.471517Z"
    },
    "papermill": {
     "duration": 0.024598,
     "end_time": "2023-12-07T04:27:17.474410",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.449812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UBCDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)  # Reset index\n",
    "        self.file_names = self.df['file_path'].values\n",
    "        self.labels = self.df['label'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        img = torch.tensor(img, dtype=torch.float)  # Convert image to torch tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Ensure label is a long tensor\n",
    "        \n",
    "        return img, label  # Return image and label as a tuple\n",
    "\n",
    "class UBCDataset2(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transforms = transforms\n",
    "        self.sxs = df[\"sx\"].values\n",
    "        self.exs = df[\"ex\"].values\n",
    "        self.sys = df[\"sy\"].values\n",
    "        self.eys = df[\"ey\"].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        sx = self.sxs[index]\n",
    "        ex = self.exs[index]\n",
    "        sy = self.sys[index]\n",
    "        ey = self.eys[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        img = img[ sy:ey, sx:ex, : ]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        img = torch.tensor(img, dtype=torch.float)  # Convert image to torch tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Ensure label is a long tensor\n",
    "        \n",
    "        return img, label  # Return image and label as a tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d198085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.493081Z",
     "iopub.status.busy": "2023-12-07T04:27:17.492815Z",
     "iopub.status.idle": "2023-12-07T04:27:17.508415Z",
     "shell.execute_reply": "2023-12-07T04:27:17.507529Z"
    },
    "papermill": {
     "duration": 0.027927,
     "end_time": "2023-12-07T04:27:17.510681",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.482754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_transforms[\"OR\"] = A.Compose([\\n    A.Resize(CONFIG[\\'img_size\\'], CONFIG[\\'img_size\\']),\\n    A.RandomResizedCrop(int(CONFIG[\\'img_size\\']), int(CONFIG[\\'img_size\\'])),\\n    A.Normalize(\\n        mean=[0.485, 0.456, 0.406],\\n        std=[0.229, 0.224, 0.225],\\n        max_pixel_value=255.0,\\n        p=1.0\\n    ),\\n    ToTensorV2()], p=1.)\\n\\ndata_transforms[\"AR\"] = A.Compose([\\n    A.Resize(CONFIG[\\'img_size\\'], CONFIG[\\'img_size\\']),\\n    A.RandomResizedCrop(int(CONFIG[\\'img_size\\']), int(CONFIG[\\'img_size\\'])),\\n    A.HorizontalFlip(p=0.5),\\n    A.VerticalFlip(p=0.5),\\n    A.RandomBrightnessContrast(p=0.75),\\n    A.ShiftScaleRotate(p=0.75),\\n    A.OneOf([\\n    A.GaussNoise(var_limit=[10, 50]),\\n    A.GaussianBlur(),\\n    A.MotionBlur(),\\n    ], p=0.4),\\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\\n    A.CoarseDropout(max_holes=1, max_width=int(CONFIG[\\'img_size\\']* 0.3), max_height=int(CONFIG[\\'img_size\\']* 0.3),\\n    mask_fill_value=0, p=0.5),\\n    A.Normalize(\\n        mean=[0.485, 0.456, 0.406],\\n        std=[0.229, 0.224, 0.225],\\n        max_pixel_value=255.0,\\n        p=1.0\\n    ),\\n    ToTensorV2()], p=1.)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}\n",
    "\n",
    "data_transforms[\"O\"] = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "        max_pixel_value=255.0,\n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()], p=1.)\n",
    "\n",
    "data_transforms[\"A\"] = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.75),\n",
    "    A.ShiftScaleRotate(p=0.75),\n",
    "    A.OneOf([\n",
    "    A.GaussNoise(var_limit=[10, 50]),\n",
    "    A.GaussianBlur(),\n",
    "    A.MotionBlur(),\n",
    "    ], p=0.4),\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "    A.CoarseDropout(max_holes=1, max_width=int(CONFIG['img_size']* 0.3), max_height=int(CONFIG['img_size']* 0.3),\n",
    "    mask_fill_value=0, p=0.5),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "        max_pixel_value=255.0,\n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()], p=1.)\n",
    "\n",
    "'''data_transforms[\"OR\"] = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.RandomResizedCrop(int(CONFIG['img_size']), int(CONFIG['img_size'])),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "        max_pixel_value=255.0,\n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()], p=1.)\n",
    "\n",
    "data_transforms[\"AR\"] = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.RandomResizedCrop(int(CONFIG['img_size']), int(CONFIG['img_size'])),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.75),\n",
    "    A.ShiftScaleRotate(p=0.75),\n",
    "    A.OneOf([\n",
    "    A.GaussNoise(var_limit=[10, 50]),\n",
    "    A.GaussianBlur(),\n",
    "    A.MotionBlur(),\n",
    "    ], p=0.4),\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "    A.CoarseDropout(max_holes=1, max_width=int(CONFIG['img_size']* 0.3), max_height=int(CONFIG['img_size']* 0.3),\n",
    "    mask_fill_value=0, p=0.5),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "        max_pixel_value=255.0,\n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()], p=1.)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d3cf5ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.528907Z",
     "iopub.status.busy": "2023-12-07T04:27:17.528655Z",
     "iopub.status.idle": "2023-12-07T04:27:17.534411Z",
     "shell.execute_reply": "2023-12-07T04:27:17.533536Z"
    },
    "papermill": {
     "duration": 0.017172,
     "end_time": "2023-12-07T04:27:17.536366",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.519194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  def classify(self, images):\\n      with torch.no_grad():\\n          _, probs = self.forward(images)\\n          probabilities = torch.nn.functional.softmax(probs, dim=1)\\n          max_prob, preds = torch.max(probabilities, 1)\\n          classifications = torch.where(\\n              max_prob > self.threshold,\\n              preds,\\n              torch.full_like(preds, self.num_classes)  # 'Other' class\\n          )\\n          return classifications\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  '''  def classify(self, images):\n",
    "        with torch.no_grad():\n",
    "            _, probs = self.forward(images)\n",
    "            probabilities = torch.nn.functional.softmax(probs, dim=1)\n",
    "            max_prob, preds = torch.max(probabilities, 1)\n",
    "            classifications = torch.where(\n",
    "                max_prob > self.threshold,\n",
    "                preds,\n",
    "                torch.full_like(preds, self.num_classes)  # 'Other' class\n",
    "            )\n",
    "            return classifications'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5cb709f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.555399Z",
     "iopub.status.busy": "2023-12-07T04:27:17.555134Z",
     "iopub.status.idle": "2023-12-07T04:27:17.587091Z",
     "shell.execute_reply": "2023-12-07T04:27:17.586506Z"
    },
    "papermill": {
     "duration": 0.044002,
     "end_time": "2023-12-07T04:27:17.589065",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.545063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n",
    "    \n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=5, threshold=0.3, pretrained=True, checkpoint_path=None):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.model = timm.create_model('tf_efficientnet_b0', checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b0/1/tf_efficientnet_b0_aa-827b6e33.pth')\n",
    "        # Keeping the feature extraction part and removing the classification head\n",
    "        self.threshold = threshold\n",
    "        self.num_classes = num_classes\n",
    "        self.pooling = GeM()\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Adjust p as necessary\n",
    "        self.batch_norm = nn.BatchNorm1d(in_features)  # Batch normalization\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        pooled_features = self.dropout(pooled_features)  # Dropout\n",
    "        # Check the batch size and skip batch normalization if the batch size is 1\n",
    "        if pooled_features.size(0) > 1:\n",
    "            pooled_features = self.batch_norm(pooled_features)\n",
    "        else:\n",
    "            # Optionally print a warning or handle the singleton batch differently\n",
    "            pass\n",
    "        output = self.linear(pooled_features)\n",
    "        probs = self.softmax(output)\n",
    "        return output, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        return loss\n",
    "        \n",
    "    def classify(self, images):\n",
    "        with torch.no_grad():\n",
    "            _, probs = self.forward(images)\n",
    "            max_prob, preds = torch.max(probs, 1)\n",
    "            # Apply threshold to determine 'Other'\n",
    "            classifications = torch.where(\n",
    "                max_prob > self.threshold,\n",
    "                preds,\n",
    "                #torch.tensor(self.num_classes)\n",
    "                torch.full_like(preds, self.num_classes)  # 'Other' class\n",
    "            )\n",
    "            return classifications\n",
    "\n",
    "class EfficientNet2(nn.Module):\n",
    "    def __init__(self, num_classes=5, threshold=0.3, pretrained=True, checkpoint_path=None):\n",
    "        super(EfficientNet2, self).__init__()\n",
    "        self.model = timm.create_model('tf_efficientnetv2_s_in21ft1k', pretrained=False)\n",
    "        pretrained_dict=torch.load('/kaggle/input/effi-v2/v2s.pth')\n",
    "        self.model.load_state_dict(pretrained_dict)\n",
    "        # Keeping the feature extraction part and removing the classification head\n",
    "        self.threshold = threshold\n",
    "        self.num_classes = num_classes\n",
    "        self.pooling = GeM()\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Adjust p as necessary\n",
    "        self.batch_norm = nn.BatchNorm1d(in_features)  # Batch normalization\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        pooled_features = self.dropout(pooled_features)  # Dropout\n",
    "        # Check the batch size and skip batch normalization if the batch size is 1\n",
    "        if pooled_features.size(0) > 1:\n",
    "            pooled_features = self.batch_norm(pooled_features)\n",
    "        else:\n",
    "            # Optionally print a warning or handle the singleton batch differently\n",
    "            pass\n",
    "        output = self.linear(pooled_features)\n",
    "        probs = self.softmax(output)\n",
    "        return output, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        return loss\n",
    "        \n",
    "    def classify(self, images):\n",
    "        with torch.no_grad():\n",
    "            _, probs = self.forward(images)\n",
    "            max_prob, preds = torch.max(probs, 1)\n",
    "            # Apply threshold to determine 'Other'\n",
    "            classifications = torch.where(\n",
    "                max_prob > self.threshold,\n",
    "                preds,\n",
    "                #torch.tensor(self.num_classes)\n",
    "                torch.full_like(preds, self.num_classes)  # 'Other' class\n",
    "            )\n",
    "            return classifications        \n",
    "        \n",
    "\n",
    "class SAM(Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is not None:\n",
    "                    self.state[p][\"e_w\"] = p.grad * scale\n",
    "                    p.add_(self.state[p][\"e_w\"])  # climb to the local maximum\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is not None:\n",
    "                    p.sub_(self.state[p][\"e_w\"])  # get back to the original parameters\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        raise NotImplementedError(\"SAM doesn't work like the other optimizers, you should first call `first_step` and the `second_step`; see the documentation for more info.\")\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        gradients = [\n",
    "            p.grad.norm(p=2).to(shared_device)\n",
    "            for group in self.param_groups for p in group[\"params\"]\n",
    "            if p.grad is not None\n",
    "        ]\n",
    "\n",
    "        if not gradients:  # If there are no gradients, return a dummy value (e.g., 0)\n",
    "            return torch.tensor(0.0).to(shared_device)\n",
    "\n",
    "        norm = torch.norm(torch.stack(gradients), p=2)\n",
    "        return norm\n",
    "\n",
    "#base_optimizer = torch.optim.SGD\n",
    "#optimizer = SAM(model.parameters(), base_optimizer, lr=CONFIG[\"lr\"], momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d3501b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.607406Z",
     "iopub.status.busy": "2023-12-07T04:27:17.607153Z",
     "iopub.status.idle": "2023-12-07T04:27:17.614892Z",
     "shell.execute_reply": "2023-12-07T04:27:17.614021Z"
    },
    "papermill": {
     "duration": 0.018993,
     "end_time": "2023-12-07T04:27:17.616795",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.597802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from torch.utils.data import DataLoader, ConcatDataset, random_split\\n\\nif training:\\n    # Concatenating the datasets\\n    #O_train_dataset = UBCDataset2(df_train, transforms=data_transforms['O'])\\n    A_train_dataset = UBCDataset(train_df, transforms=data_transforms['A'])\\n    #OR_train_dataset = UBCDataset2(df_train, transforms=data_transforms['OR'])\\n    #AR_train_dataset = UBCDataset2(df_train, transforms=data_transforms['AR'])\\n\\n    combined_dataset = ConcatDataset([A_train_dataset])#, O_train_dataset, OR_train_dataset, AR_train_dataset])\\n\\n    # Splitting the combined dataset into 5 parts\\n    total_size = len(combined_dataset)\\n    part_size = total_size // 5\\n    remainder = total_size % 5\\n    lengths = [part_size + (1 if i < remainder else 0) for i in range(5)]\\n\\n    split_datasets = random_split(combined_dataset, lengths)\\n\\n    train_dic = {}\\n    valid_dic = {}\\n\\n    for i in range(5):\\n        temp_v = split_datasets[i]\\n        temp_t = [split_datasets[j] for j in range(5) if j != i]\\n        train_dic[f'v{i+1}'] = DataLoader(ConcatDataset(temp_t), batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\\n        valid_dic[f'v{i+1}'] = DataLoader(temp_v, batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "\n",
    "if training:\n",
    "    # Concatenating the datasets\n",
    "    #O_train_dataset = UBCDataset2(df_train, transforms=data_transforms['O'])\n",
    "    A_train_dataset = UBCDataset(train_df, transforms=data_transforms['A'])\n",
    "    #OR_train_dataset = UBCDataset2(df_train, transforms=data_transforms['OR'])\n",
    "    #AR_train_dataset = UBCDataset2(df_train, transforms=data_transforms['AR'])\n",
    "\n",
    "    combined_dataset = ConcatDataset([A_train_dataset])#, O_train_dataset, OR_train_dataset, AR_train_dataset])\n",
    "\n",
    "    # Splitting the combined dataset into 5 parts\n",
    "    total_size = len(combined_dataset)\n",
    "    part_size = total_size // 5\n",
    "    remainder = total_size % 5\n",
    "    lengths = [part_size + (1 if i < remainder else 0) for i in range(5)]\n",
    "\n",
    "    split_datasets = random_split(combined_dataset, lengths)\n",
    "\n",
    "    train_dic = {}\n",
    "    valid_dic = {}\n",
    "\n",
    "    for i in range(5):\n",
    "        temp_v = split_datasets[i]\n",
    "        temp_t = [split_datasets[j] for j in range(5) if j != i]\n",
    "        train_dic[f'v{i+1}'] = DataLoader(ConcatDataset(temp_t), batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\n",
    "        valid_dic[f'v{i+1}'] = DataLoader(temp_v, batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1931ba4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.636989Z",
     "iopub.status.busy": "2023-12-07T04:27:17.636721Z",
     "iopub.status.idle": "2023-12-07T04:27:17.648306Z",
     "shell.execute_reply": "2023-12-07T04:27:17.647462Z"
    },
    "papermill": {
     "duration": 0.024115,
     "end_time": "2023-12-07T04:27:17.650328",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.626213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "split=3\n",
    "if training:\n",
    "    # Initialize dictionaries to store the data loaders for each fold\n",
    "    train_dic = {}\n",
    "    valid_dic = {}\n",
    "\n",
    "    # Manually splitting the DataFrame into 5 parts\n",
    "    df_splits = np.array_split(train_df.sample(frac=1, random_state=42), split)\n",
    "\n",
    "    # Iterate over each split to create combined datasets\n",
    "    for i in range(split):\n",
    "        # Collecting all parts except the ith for training\n",
    "        train_df_split = pd.concat([df_splits[j] for j in range(split) if j != i]).reset_index(drop=True)\n",
    "\n",
    "        # The ith part is used for validation\n",
    "        valid_df_split = df_splits[i]\n",
    "        \n",
    "        dfs = []\n",
    "        for (file_path, image_id,label) in zip(train_df_split[\"file_path\"], train_df_split[\"image_id\"],train_df_split['label']):\n",
    "            dfs.append(get_cropped_images(file_path, image_id,label))\n",
    "\n",
    "        df_train = pd.concat(dfs)\n",
    "        df_train = df_train.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\n",
    "        \n",
    "        dfs = []\n",
    "        for (file_path, image_id,label) in zip(valid_df_split[\"file_path\"], valid_df_split[\"image_id\"],valid_df_split['label']):\n",
    "            dfs.append( get_cropped_images(file_path, image_id,label) )\n",
    "\n",
    "        df_crop = pd.concat(dfs)\n",
    "        df_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\n",
    "\n",
    "        # Define the datasets with different transformations for the splits\n",
    "        train_datasets = {\n",
    "            #'A': UBCDataset2(df_train, transforms=data_transforms['A']),\n",
    "            'O': UBCDataset2(df_train, transforms=data_transforms['O']),\n",
    "            # Add other datasets as needed\n",
    "        }\n",
    "\n",
    "        valid_datasets = {\n",
    "            'A': UBCDataset2(df_crop, transforms=data_transforms['valid']),\n",
    "            # Add other datasets as needed\n",
    "        }\n",
    "\n",
    "        # Creating DataLoaders for the ith fold\n",
    "        train_dic[f'v{i+1}'] = DataLoader(ConcatDataset(list(train_datasets.values())), batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\n",
    "        valid_dic[f'v{i+1}'] = DataLoader(ConcatDataset(list(valid_datasets.values())), batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b4e0bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.669808Z",
     "iopub.status.busy": "2023-12-07T04:27:17.669520Z",
     "iopub.status.idle": "2023-12-07T04:27:17.686234Z",
     "shell.execute_reply": "2023-12-07T04:27:17.685550Z"
    },
    "papermill": {
     "duration": 0.028844,
     "end_time": "2023-12-07T04:27:17.688265",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.659421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    clear_output(wait=True)\n",
    "    #device = cuda.get_current_device(); device.reset()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Loss and optimizer\n",
    "    class_weights = torch.Tensor(list(weight_balance.values())).to(CONFIG['device'])\n",
    "    # Training loop\n",
    "    for i in range(split):\n",
    "        model = EfficientNet2(num_classes=CONFIG['num_classes'], threshold = CONFIG[\"Threshold\"])\n",
    "        model.to(CONFIG['device']);\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)#, weight_decay=0.3)\n",
    "        base_optimizer = torch.optim.Adam\n",
    "        optimizer2 = SAM(model.parameters(), base_optimizer, lr=CONFIG[\"lr\"])\n",
    "        best_loss = float('inf')\n",
    "        obj=True\n",
    "        epoch=0\n",
    "        train_loss_dic={}\n",
    "        valid_loss_dic={}\n",
    "        while obj:  # Number of epochs\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            model.train()  # Set the model to training mode\n",
    "            print(f'v{i+1}')\n",
    "            for data in tqdm(train_dic[f'v{i+1}']):\n",
    "                images, labels = data[0].to(CONFIG['device']), data[1].to(CONFIG['device'])\n",
    "                # Forward pass\n",
    "                classifications,_ = model(images)\n",
    "                loss = criterion(classifications, labels)\n",
    "                # Backward pass and optimization\n",
    "                if True: #valid_loss == 0.0 or valid_loss > 1.2:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    optimizer2.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer2.first_step(zero_grad=True)\n",
    "                    criterion(model(images)[0], labels).backward()  # You can use outputs from the first forward pass \n",
    "                    optimizer2.second_step()\n",
    "                train_loss+=loss.item() * len(data)\n",
    "                \n",
    "            train_loss = train_loss/len(train_dic[f'v{i+1}'].sampler)\n",
    "            train_loss_dic[f't{epoch+1}']=train_loss\n",
    "            # Validation loop\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():  # Disable gradient computation\n",
    "                for data in tqdm(valid_dic[f'v{i+1}']):\n",
    "                    images, labels = data[0].to(CONFIG['device']), data[1].to(CONFIG['device'])\n",
    "                    classifications, _ = model(images)\n",
    "                    v_loss = criterion(classifications, labels)\n",
    "                    valid_loss += v_loss.item()\n",
    "            valid_loss = valid_loss/len(valid_dic[f'v{i+1}'].sampler)\n",
    "            valid_loss_dic[f't{epoch+1}']=valid_loss\n",
    "\n",
    "            # Save the model if validation loss has decreased\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f'best_model_weights{i+1}.pth')\n",
    "                print('Best Model Saved')\n",
    "\n",
    "            # Check for stopping condition\n",
    "            if epoch == 0:\n",
    "                cri=valid_loss*0.7\n",
    "            if best_loss < cri:\n",
    "                obj=False\n",
    "            if epoch > 5 and valid_loss_dic[f't{epoch-1}']-valid_loss_dic[f't{epoch}'] < 0 and valid_loss_dic[f't{epoch}']-valid_loss_dic[f't{epoch+1}'] < 0 and valid_loss_dic[f't{epoch-2}']-valid_loss_dic[f't{epoch-1}'] < 0:\n",
    "                obj=False\n",
    "            epoch += 1\n",
    "            print(f'Epoch [{epoch}], Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}, fold: {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa1bc7c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.707690Z",
     "iopub.status.busy": "2023-12-07T04:27:17.707412Z",
     "iopub.status.idle": "2023-12-07T04:27:17.715122Z",
     "shell.execute_reply": "2023-12-07T04:27:17.714293Z"
    },
    "papermill": {
     "duration": 0.019813,
     "end_time": "2023-12-07T04:27:17.717056",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.697243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef most_frequent_element(arr):\\n    counter = Counter(arr)\\n    most_common = counter.most_common(1)[0][0]\\n    return most_common\\nmodel = EfficientNet(num_classes=CONFIG[\\'num_classes\\'], threshold=CONFIG[\"Threshold\"])\\nmodel.to(CONFIG[\\'device\\']);\\n\\ndfs = []\\nfor (file_path, image_id,label) in zip(test_df[\"file_path\"], test_df[\"image_id\"],test_df[\\'label\\']):\\n    dfs.append( get_cropped_images(file_path, image_id,label) )\\n\\ndf_crop = pd.concat(dfs)\\ndf_crop[\"label\"] = 0 # dummy\\n\\ndf_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\\ntest_dataset = UBCDataset2(df_crop, transforms=data_transforms[\\'valid\\'])\\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG[\\'batch\\'], num_workers=2, shuffle=False, pin_memory=True)\\n\\n# Initialize a list to store predictions from all models\\nall_model_preds = []\\nall_model_prob = []\\nfor i in range(5):\\n    model.load_state_dict(torch.load(f\\'/kaggle/input/weights4/best_model_weights{i+1}.pth\\',map_location=torch.device(CONFIG[\\'device\\'])))\\n    model.eval()\\n    model_preds = []\\n    model_prob = []\\n    with torch.no_grad():\\n        for images, _ in test_loader:        \\n            images = images.to(CONFIG[\\'device\\'], dtype=torch.float)\\n\\n            _, probabilities = model(images)\\n            model_prob.append(probabilities)\\n            _, predicted = torch.max(probabilities, 1)\\n            model_preds.extend(predicted.detach().cpu().numpy())\\n    all_model_prob.append(model_prob)\\n    all_model_preds.append(model_preds)\\n\\n# Transpose to align predictions for each data point across models\\ncombined_pred = np.array(all_model_preds).T\\n\\n# Find the most frequent predictions for each data point\\naggregated_labels = [most_frequent_element(pred_list) for pred_list in combined_pred]\\n\\n# Assuming label_mapping is defined to map labels to actual class names\\npred_labels = [label_mapping[label] for label in aggregated_labels]\\n\\n# Assuming df_sub is your submission dataframe\\ndf_sub[\"label\"] = pred_labels\\ndf_sub.to_csv(\"submission.csv\", index=False)\\n\\nif training:\\n    print(\"Probabilities:\", all_model_prob)\\n    print(\"Predictions:\", combined_pred)\\n    print(\"Aggregated Labels:\", pred_labels)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "def most_frequent_element(arr):\n",
    "    counter = Counter(arr)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    return most_common\n",
    "model = EfficientNet(num_classes=CONFIG['num_classes'], threshold=CONFIG[\"Threshold\"])\n",
    "model.to(CONFIG['device']);\n",
    "\n",
    "dfs = []\n",
    "for (file_path, image_id,label) in zip(test_df[\"file_path\"], test_df[\"image_id\"],test_df['label']):\n",
    "    dfs.append( get_cropped_images(file_path, image_id,label) )\n",
    "\n",
    "df_crop = pd.concat(dfs)\n",
    "df_crop[\"label\"] = 0 # dummy\n",
    "\n",
    "df_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\n",
    "test_dataset = UBCDataset2(df_crop, transforms=data_transforms['valid'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Initialize a list to store predictions from all models\n",
    "all_model_preds = []\n",
    "all_model_prob = []\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load(f'/kaggle/input/weights4/best_model_weights{i+1}.pth',map_location=torch.device(CONFIG['device'])))\n",
    "    model.eval()\n",
    "    model_preds = []\n",
    "    model_prob = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:        \n",
    "            images = images.to(CONFIG['device'], dtype=torch.float)\n",
    "\n",
    "            _, probabilities = model(images)\n",
    "            model_prob.append(probabilities)\n",
    "            _, predicted = torch.max(probabilities, 1)\n",
    "            model_preds.extend(predicted.detach().cpu().numpy())\n",
    "    all_model_prob.append(model_prob)\n",
    "    all_model_preds.append(model_preds)\n",
    "\n",
    "# Transpose to align predictions for each data point across models\n",
    "combined_pred = np.array(all_model_preds).T\n",
    "\n",
    "# Find the most frequent predictions for each data point\n",
    "aggregated_labels = [most_frequent_element(pred_list) for pred_list in combined_pred]\n",
    "\n",
    "# Assuming label_mapping is defined to map labels to actual class names\n",
    "pred_labels = [label_mapping[label] for label in aggregated_labels]\n",
    "\n",
    "# Assuming df_sub is your submission dataframe\n",
    "df_sub[\"label\"] = pred_labels\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "if training:\n",
    "    print(\"Probabilities:\", all_model_prob)\n",
    "    print(\"Predictions:\", combined_pred)\n",
    "    print(\"Aggregated Labels:\", pred_labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170df02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.735393Z",
     "iopub.status.busy": "2023-12-07T04:27:17.735140Z",
     "iopub.status.idle": "2023-12-07T04:27:17.742549Z",
     "shell.execute_reply": "2023-12-07T04:27:17.741670Z"
    },
    "papermill": {
     "duration": 0.01875,
     "end_time": "2023-12-07T04:27:17.744453",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.725703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_dataset = UBCDataset(train_df, transforms=data_transforms[\\'N\\'])\\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG[\\'batch\\'], num_workers=2, shuffle=False, pin_memory=True)\\n\\n\\n# Initialize model\\nmodel = EfficientNet2(num_classes=CONFIG[\\'num_classes\\'], threshold=0.3)\\n#model = ViTModel(threshold=0.3)\\nmodel.to(CONFIG[\\'device\\']);\\nclear_output(wait=True)\\n\\n# Loss and optimizer\\nclass_weights = torch.Tensor(list(weight_balance.values())).to(CONFIG[\\'device\\'])\\n\\n# Loss with class weights\\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\\noptimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\\n#base_optimizer = torch.optim.Adam\\n#optimizer2 = SAM(model.parameters(), base_optimizer, lr=CONFIG[\"lr\"])\\n\\nbest_loss = float(\\'inf\\')\\nobj=True\\nepoch=0\\n# Training loop\\nwhile obj:  # Number of epochs\\n    train_loss=0.0\\n    model.train()  # Set the model to training mode\\n    for data in tqdm(train_loader):\\n        images, labels = data[0].to(CONFIG[\\'device\\']), data[1].to(CONFIG[\\'device\\'])\\n        # Forward pass\\n        classifications,_ = model(images)\\n        loss = criterion(classifications, labels)\\n        \\n        # Backward pass and optimization\\n        optimizer.zero_grad()\\n        #optimizer2.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        train_loss+=loss.item() * len(data)\\n    train_loss = train_loss/len(train_loader.sampler)\\n        #optimizer2.first_step(zero_grad=True)\\n        \\n        #criterion(model(images)[0], labels).backward()  # You can use outputs from the first forward pass\\n        #optimizer2.second_step()\\n        \\n    if train_loss < best_loss:\\n        best_loss = train_loss\\n        torch.save(model.state_dict(), \\'best_model_weights.pth\\')\\n        print(\\'Best Model Saved\\')\\n    if best_loss < 0.02:\\n        obj=False\\n    epoch+=1\\n        \\n    print(f\\'Epoch [{epoch}], Loss: {train_loss:.4f}\\')'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_dataset = UBCDataset(train_df, transforms=data_transforms['N'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = EfficientNet2(num_classes=CONFIG['num_classes'], threshold=0.3)\n",
    "#model = ViTModel(threshold=0.3)\n",
    "model.to(CONFIG['device']);\n",
    "clear_output(wait=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "class_weights = torch.Tensor(list(weight_balance.values())).to(CONFIG['device'])\n",
    "\n",
    "# Loss with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "#base_optimizer = torch.optim.Adam\n",
    "#optimizer2 = SAM(model.parameters(), base_optimizer, lr=CONFIG[\"lr\"])\n",
    "\n",
    "best_loss = float('inf')\n",
    "obj=True\n",
    "epoch=0\n",
    "# Training loop\n",
    "while obj:  # Number of epochs\n",
    "    train_loss=0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "    for data in tqdm(train_loader):\n",
    "        images, labels = data[0].to(CONFIG['device']), data[1].to(CONFIG['device'])\n",
    "        # Forward pass\n",
    "        classifications,_ = model(images)\n",
    "        loss = criterion(classifications, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        #optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item() * len(data)\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "        #optimizer2.first_step(zero_grad=True)\n",
    "        \n",
    "        #criterion(model(images)[0], labels).backward()  # You can use outputs from the first forward pass\n",
    "        #optimizer2.second_step()\n",
    "        \n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save(model.state_dict(), 'best_model_weights.pth')\n",
    "        print('Best Model Saved')\n",
    "    if best_loss < 0.02:\n",
    "        obj=False\n",
    "    epoch+=1\n",
    "        \n",
    "    print(f'Epoch [{epoch}], Loss: {train_loss:.4f}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0979adf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.763509Z",
     "iopub.status.busy": "2023-12-07T04:27:17.763221Z",
     "iopub.status.idle": "2023-12-07T04:27:17.769892Z",
     "shell.execute_reply": "2023-12-07T04:27:17.768992Z"
    },
    "papermill": {
     "duration": 0.01916,
     "end_time": "2023-12-07T04:27:17.772479",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.753319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport timm\\n\\ndfs = []\\nfor (file_path, image_id,label) in zip(test_df[\"file_path\"], test_df[\"image_id\"],test_df[\\'label\\']):\\n    dfs.append( get_cropped_images(file_path, image_id,label) )\\n\\ndf_crop = pd.concat(dfs)\\ndf_crop[\"label\"] = 0 # dummy\\n\\ndf_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\\ntest_dataset = UBCDataset2(df_crop, transforms=data_transforms[\\'valid\\'])\\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG[\\'batch\\'], num_workers=2, shuffle=False, pin_memory=True)\\n\\nmodel = EfficientNet2(num_classes=CONFIG[\\'num_classes\\'], threshold=0.3)\\nmodel.to(CONFIG[\\'device\\']);\\nmodel.load_state_dict(torch.load(\\'/kaggle/input/weights5/best_model_weights3.pth\\'))\\n# Test the model\\npreds = []\\nwith torch.no_grad():\\n    bar = tqdm(enumerate(test_loader), total=len(test_loader))\\n    for step, data in bar:        \\n        images = data[0].to(CONFIG[\\'device\\'], dtype=torch.float)\\n        batch_size = images.size(0)\\n\\n        _, probabilities = model(images)  # Unpack the tuple here\\n        _, predicted = torch.max(probabilities, 1)  # Use probabilities for predictions\\n        preds.append(predicted.detach().cpu().numpy())\\n\\npreds = np.concatenate(preds).flatten()\\npred_labels = [label_mapping[label] for label in preds]\\ndf_sub[\"label\"] = pred_labels\\ndf_sub.to_csv(\"submission.csv\", index=False)\\nprint(predicted)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "import timm\n",
    "\n",
    "dfs = []\n",
    "for (file_path, image_id,label) in zip(test_df[\"file_path\"], test_df[\"image_id\"],test_df['label']):\n",
    "    dfs.append( get_cropped_images(file_path, image_id,label) )\n",
    "\n",
    "df_crop = pd.concat(dfs)\n",
    "df_crop[\"label\"] = 0 # dummy\n",
    "\n",
    "df_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\n",
    "test_dataset = UBCDataset2(df_crop, transforms=data_transforms['valid'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\n",
    "\n",
    "model = EfficientNet2(num_classes=CONFIG['num_classes'], threshold=0.3)\n",
    "model.to(CONFIG['device']);\n",
    "model.load_state_dict(torch.load('/kaggle/input/weights5/best_model_weights3.pth'))\n",
    "# Test the model\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, data in bar:        \n",
    "        images = data[0].to(CONFIG['device'], dtype=torch.float)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        _, probabilities = model(images)  # Unpack the tuple here\n",
    "        _, predicted = torch.max(probabilities, 1)  # Use probabilities for predictions\n",
    "        preds.append(predicted.detach().cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds).flatten()\n",
    "pred_labels = [label_mapping[label] for label in preds]\n",
    "df_sub[\"label\"] = pred_labels\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "print(predicted)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65165b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T04:27:17.791882Z",
     "iopub.status.busy": "2023-12-07T04:27:17.791614Z",
     "iopub.status.idle": "2023-12-07T04:27:30.816554Z",
     "shell.execute_reply": "2023-12-07T04:27:30.815362Z"
    },
    "papermill": {
     "duration": 13.03716,
     "end_time": "2023-12-07T04:27:30.818735",
     "exception": false,
     "start_time": "2023-12-07T04:27:17.781575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.94s/it]\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for (file_path, image_id,label) in zip(test_df[\"file_path\"], test_df[\"image_id\"],test_df['label']):\n",
    "    dfs.append( get_cropped_images(file_path, image_id,label) )\n",
    "\n",
    "df_crop = pd.concat(dfs)\n",
    "df_crop[\"label\"] = 0 # dummy\n",
    "\n",
    "df_crop = df_crop.drop_duplicates(subset=[\"image_id\", \"sx\", \"ex\", \"sy\", \"ey\"]).reset_index(drop=True)\n",
    "test_dataset = UBCDataset2(df_crop, transforms=data_transforms['valid'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch'], num_workers=2, shuffle=False, pin_memory=True)\n",
    "\n",
    "model = EfficientNet2(num_classes=CONFIG['num_classes'], threshold=0.3)\n",
    "model.to(CONFIG['device']);\n",
    "model.load_state_dict(torch.load('/kaggle/input/weights5/best_model_weights3.pth'))\n",
    "\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, data in bar:        \n",
    "        images = data[0].to(CONFIG[\"device\"], dtype=torch.float)        \n",
    "        batch_size = images.size(0)\n",
    "        _, probabilities = model(images)  # Unpack the tuple here\n",
    "        _, predicted = torch.max(probabilities, 1)  # Use probabilities for predictions\n",
    "        preds.append(predicted.detach().cpu().numpy())\n",
    "\n",
    "preds = np.vstack(preds)\n",
    "\n",
    "for i in range(preds.shape[-1]):\n",
    "    df_crop[f\"cat{i}\"] = preds[:, i]\n",
    "\n",
    "dict_label = {}\n",
    "for image_id, gdf in df_crop.groupby(\"image_id\"):\n",
    "    dict_label[image_id] = np.argmax( gdf[ [f\"cat{i}\" for i in range(preds.shape[-1])] ].values.max(axis=0) )\n",
    "    #dict_label[image_id] = np.argmax( gdf[ [f\"cat{i}\" for i in range(preds.shape[-1])] ].values.mean(axis=0) )\n",
    "preds = np.array( [ dict_label[image_id] for image_id in test_df[\"image_id\"].values ] )\n",
    "\n",
    "pred_labels = [label_mapping[label] for label in preds]\n",
    "df_sub[\"label\"] = pred_labels\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607cca8",
   "metadata": {
    "papermill": {
     "duration": 0.009599,
     "end_time": "2023-12-07T04:27:30.838277",
     "exception": false,
     "start_time": "2023-12-07T04:27:30.828678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6924515,
     "sourceId": 45867,
     "sourceType": "competition"
    },
    {
     "datasetId": 3833517,
     "sourceId": 6640479,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4036519,
     "sourceId": 7019790,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4041642,
     "sourceId": 7027265,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4042278,
     "sourceId": 7028227,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4047635,
     "sourceId": 7035781,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4056972,
     "sourceId": 7049679,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4111744,
     "sourceId": 7127432,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 846,
     "sourceId": 992,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2656,
     "sourceId": 3729,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2664,
     "sourceId": 3737,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.666867,
   "end_time": "2023-12-07T04:27:33.175885",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-07T04:27:03.509018",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
